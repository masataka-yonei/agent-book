{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tXirKYf9Jje"
      },
      "source": [
        "# 5. LangChain Expression Language（LCEL）徹底解説\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
          "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
          "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
          "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
        },
        "id": "fkz3grmx9Jjf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OY6BX7ZD9Jjg",
        "outputId": "7f6f4993-1323-4932-b4f1-97ab3647bfa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core==0.3.0 in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-openai==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: langchain-community==0.3.0 in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.3.0) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.117 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.3.0) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.3.0) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.3.0) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.3.0) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.0) (1.57.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.0) (0.8.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (3.23.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core==0.3.0) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.0) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.0) (2024.11.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai==0.2.0) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 langchain-community==0.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2w-wXA-9Jjg"
      },
      "source": [
        "## 5.1. Runnable と RunnableSequence―LCEL の最も基本的な構成要素\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:12.290335Z",
          "iopub.status.busy": "2024-06-28T02:33:12.290156Z",
          "iopub.status.idle": "2024-06-28T02:33:12.344661Z",
          "shell.execute_reply": "2024-06-28T02:33:12.344241Z"
        },
        "id": "P0UE9Mbq9Jjg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーが入力した料理のレシピを考えてください。\"),\n",
        "        (\"human\", \"{dish}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:12.346689Z",
          "iopub.status.busy": "2024-06-28T02:33:12.346510Z",
          "iopub.status.idle": "2024-06-28T02:33:21.108437Z",
          "shell.execute_reply": "2024-06-28T02:33:21.108007Z"
        },
        "id": "Md0KpT1o9Jjh",
        "outputId": "df9622de-4664-4568-e478-b4aa59150efb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "カレーのレシピをご紹介します！以下は基本的なチキンカレーのレシピです。\n",
            "\n",
            "### 材料（4人分）\n",
            "- 鶏もも肉：400g（食べやすい大きさにカット）\n",
            "- 玉ねぎ：2個（みじん切り）\n",
            "- にんにく：2片（みじん切り）\n",
            "- 生姜：1片（みじん切り）\n",
            "- トマト：1個（ざく切り）\n",
            "- カレーパウダー：大さじ2\n",
            "- クミンシード：小さじ1（お好みで）\n",
            "- ココナッツミルク：200ml（お好みで）\n",
            "- サラダ油：大さじ2\n",
            "- 塩：適量\n",
            "- 黒胡椒：適量\n",
            "- 水：400ml\n",
            "- パクチー（飾り用）：適量\n",
            "\n",
            "### 作り方\n",
            "1. **下準備**: 鶏肉に塩と黒胡椒を振りかけて下味をつけておきます。\n",
            "\n",
            "2. **玉ねぎを炒める**: 大きめの鍋にサラダ油を熱し、みじん切りにした玉ねぎを加え、中火で透明になるまで炒めます。\n",
            "\n",
            "3. **香味野菜を加える**: にんにくと生姜を加え、香りが立つまでさらに炒めます。\n",
            "\n",
            "4. **スパイスを加える**: クミンシードを加え、さらに1分ほど炒めた後、カレーパウダーを加え、全体がよく混ざるまで炒めます。\n",
            "\n",
            "5. **鶏肉を加える**: 鶏肉を鍋に加え、表面が白くなるまで炒めます。\n",
            "\n",
            "6. **トマトと水を加える**: ざく切りにしたトマトと水を加え、全体をよく混ぜます。沸騰したら、弱火にして蓋をし、約20分煮込みます。\n",
            "\n",
            "7. **ココナッツミルクを加える**: 煮込みが終わったら、ココナッツミルクを加え、さらに5分ほど煮込みます。味を見て、必要に応じて塩で調整します。\n",
            "\n",
            "8. **盛り付け**: お皿に盛り付け、パクチーを散らして完成です。\n",
            "\n",
            "### 提供方法\n",
            "ご飯やナンと一緒にお楽しみください。お好みでヨーグルトやサラダを添えると、より美味しくいただけます。\n",
            "\n",
            "ぜひお試しください！\n"
          ]
        }
      ],
      "source": [
        "prompt_value = prompt.invoke({\"dish\": \"カレー\"})\n",
        "ai_message = model.invoke(prompt_value)\n",
        "output = output_parser.invoke(ai_message)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:21.110332Z",
          "iopub.status.busy": "2024-06-28T02:33:21.110173Z",
          "iopub.status.idle": "2024-06-28T02:33:21.112634Z",
          "shell.execute_reply": "2024-06-28T02:33:21.112238Z"
        },
        "id": "cynHvooj9Jjh"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:21.114678Z",
          "iopub.status.busy": "2024-06-28T02:33:21.114418Z",
          "iopub.status.idle": "2024-06-28T02:33:26.539851Z",
          "shell.execute_reply": "2024-06-28T02:33:26.539250Z"
        },
        "id": "_GhiSw-w9Jjh",
        "outputId": "b79f189d-4b6d-41a4-9304-5ddc3513f7e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\n",
            "\n",
            "### 材料（4人分）\n",
            "- 鶏肉（もも肉または胸肉）: 400g\n",
            "- 玉ねぎ: 2個\n",
            "- にんじん: 1本\n",
            "- じゃがいも: 2個\n",
            "- カレールー: 1箱（約200g）\n",
            "- サラダ油: 大さじ2\n",
            "- 水: 800ml\n",
            "- 塩: 適量\n",
            "- 胡椒: 適量\n",
            "- お好みでガーリックパウダーや生姜: 適量\n",
            "\n",
            "### 作り方\n",
            "1. **材料の下ごしらえ**:\n",
            "   - 鶏肉は一口大に切り、塩と胡椒を振っておきます。\n",
            "   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\n",
            "\n",
            "2. **炒める**:\n",
            "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
            "   - 鶏肉を加え、表面が白くなるまで炒めます。\n",
            "\n",
            "3. **野菜を加える**:\n",
            "   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\n",
            "\n",
            "4. **煮る**:\n",
            "   - 水を加え、強火で煮立たせます。煮立ったら火を弱め、アクを取りながら約15分煮ます。\n",
            "\n",
            "5. **カレールーを加える**:\n",
            "   - カレールーを割り入れ、よく溶かします。さらに10分ほど煮込み、全体がなじんだら味を見て、必要に応じて塩や胡椒で調整します。\n",
            "\n",
            "6. **仕上げ**:\n",
            "   - お好みでガーリックパウダーや生姜を加えて風味をアップさせます。火を止めて、少し冷ますと味がなじみます。\n",
            "\n",
            "7. **盛り付け**:\n",
            "   - ご飯と一緒に盛り付けて、お好みで福神漬けやらっきょうを添えて完成です。\n",
            "\n",
            "### おすすめのトッピング\n",
            "- 煮卵\n",
            "- チーズ\n",
            "- パクチー\n",
            "\n",
            "この基本のカレーはアレンジがしやすいので、野菜や肉を変えて楽しんでください！\n"
          ]
        }
      ],
      "source": [
        "output = chain.invoke({\"dish\": \"カレー\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VxUpmT59Jjh"
      },
      "source": [
        "### Runnable の実行方法―invoke・stream・batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:26.545129Z",
          "iopub.status.busy": "2024-06-28T02:33:26.544905Z",
          "iopub.status.idle": "2024-06-28T02:33:32.478679Z",
          "shell.execute_reply": "2024-06-28T02:33:32.478134Z"
        },
        "id": "sUjKKO0R9Jjh",
        "outputId": "be5c7985-2593-46c8-9f2e-704eb2b2ccf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "カレーのレシピをご紹介します。シンプルで美味しい基本のカレーを作りましょう。\n",
            "\n",
            "### 材料（4人分）\n",
            "- 鶏肉（もも肉または胸肉）: 400g\n",
            "- 玉ねぎ: 2個\n",
            "- にんじん: 1本\n",
            "- じゃがいも: 2個\n",
            "- カレールー: 1箱（約200g）\n",
            "- サラダ油: 大さじ2\n",
            "- 水: 800ml\n",
            "- 塩: 適量\n",
            "- 胡椒: 適量\n",
            "- お好みでガーリックパウダーや生姜: 適量\n",
            "\n",
            "### 作り方\n",
            "1. **材料の下ごしらえ**:\n",
            "   - 鶏肉は一口大に切り、塩と胡椒をふっておきます。\n",
            "   - 玉ねぎは薄切り、にんじんは輪切り、じゃがいもは一口大に切ります。\n",
            "\n",
            "2. **炒める**:\n",
            "   - 大きめの鍋にサラダ油を熱し、玉ねぎを中火で炒めます。玉ねぎが透明になるまで炒めます。\n",
            "   - 鶏肉を加え、表面が白くなるまで炒めます。\n",
            "\n",
            "3. **野菜を加える**:\n",
            "   - にんじんとじゃがいもを鍋に加え、全体をよく混ぜます。\n",
            "\n",
            "4. **煮る**:\n",
            "   - 水を加え、強火で煮立たせます。煮立ったら、アクを取り除き、中火にして蓋をし、約15分煮ます。\n",
            "\n",
            "5. **カレールーを加える**:\n",
            "   - 火を止めてカレールーを加え、よく溶かします。再び弱火にし、10分ほど煮込みます。お好みでガーリックパウダーや生姜を加えて風味を調整します。\n",
            "\n",
            "6. **仕上げ**:\n",
            "   - 味を見て、必要であれば塩で調整します。全体がなじんだら、火を止めます。\n",
            "\n",
            "7. **盛り付け**:\n",
            "   - ご飯と一緒に盛り付けて、お好みで福神漬けやらっきょうを添えて完成です。\n",
            "\n",
            "### おすすめのトッピング\n",
            "- 煮卵\n",
            "- チーズ\n",
            "- 青ねぎ\n",
            "\n",
            "この基本のカレーはアレンジがしやすいので、野菜や肉を変えて楽しんでください！"
          ]
        }
      ],
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "for chunk in chain.stream({\"dish\": \"カレー\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:32.480592Z",
          "iopub.status.busy": "2024-06-28T02:33:32.480406Z",
          "iopub.status.idle": "2024-06-28T02:33:38.976064Z",
          "shell.execute_reply": "2024-06-28T02:33:38.974376Z"
        },
        "id": "cbFLiq2z9Jji",
        "outputId": "5e2963a5-80f2-4c9a-b2f0-25c9b40fc27d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['カレーのレシピをご紹介します！以下は基本的なチキンカレーのレシピです。\\n\\n### 材料（4人分）\\n- 鶏もも肉：400g（食べやすい大きさにカット）\\n- 玉ねぎ：2個（みじん切り）\\n- にんにく：2片（みじん切り）\\n- 生姜：1片（みじん切り）\\n- トマト：1個（ざく切り）\\n- カレーパウダー：大さじ2\\n- クミンパウダー：小さじ1\\n- ココナッツミルク：200ml（お好みで）\\n- サラダ油：大さじ2\\n- 塩：適量\\n- 黒胡椒：適量\\n- 水：400ml\\n- パクチー（飾り用）：適量\\n\\n### 作り方\\n1. **下ごしらえ**: 鶏肉に塩と黒胡椒をふり、下味をつけておきます。\\n2. **玉ねぎを炒める**: 大きめの鍋にサラダ油を熱し、みじん切りにした玉ねぎを加え、透明になるまで中火で炒めます。\\n3. **香味野菜を加える**: にんにくと生姜を加え、香りが立つまでさらに炒めます。\\n4. **鶏肉を加える**: 鶏肉を鍋に加え、表面が白くなるまで炒めます。\\n5. **スパイスを加える**: カレーパウダーとクミンパウダーを加え、全体に絡めるように炒めます。\\n6. **トマトと水を加える**: ざく切りにしたトマトと水を加え、煮立たせます。アクが出たら取り除きます。\\n7. **煮込む**: 蓋をして中弱火で約20分煮込みます。鶏肉が柔らかくなったら、ココナッツミルクを加え、さらに5分煮ます。\\n8. **味を調える**: 塩で味を調整し、必要に応じてスパイスを追加します。\\n9. **盛り付け**: お皿に盛り付け、パクチーを散らして完成です。\\n\\n### 提供方法\\nご飯やナンと一緒にお楽しみください。お好みでヨーグルトやサラダを添えると、より美味しくいただけます。\\n\\nぜひお試しください！', 'うどんのレシピをご紹介します。シンプルで美味しい「かけうどん」の作り方です。\\n\\n### 材料（2人分）\\n- うどん（乾燥または生）: 2玉\\n- だし汁: 600ml\\n  - だしの素: 大さじ1（または昆布と鰹節を使って自家製だしを取る）\\n  - 水: 600ml\\n- 醤油: 大さじ2\\n- みりん: 大さじ1\\n- 塩: 少々\\n- トッピング（お好みで）:\\n  - ネギ（小口切り）\\n  - 天かす\\n  - かまぼこ\\n  - ほうれん草（茹でておく）\\n  - すりごま\\n\\n### 作り方\\n1. **だし汁を作る**:\\n   - 鍋に水を入れ、だしの素を加えて中火にかけます。自家製だしを使う場合は、昆布を水に浸けておき、沸騰直前に鰹節を加え、火を止めてしばらく置いてからこします。\\n\\n2. **うどんを茹でる**:\\n   - 別の鍋にたっぷりの水を沸かし、うどんをパッケージの指示に従って茹でます。茹で上がったら、冷水でしっかりと洗い、ぬめりを取ります。\\n\\n3. **だし汁に調味料を加える**:\\n   - 作っただし汁に醤油、みりん、塩を加え、軽く混ぜて味を整えます。\\n\\n4. **盛り付け**:\\n   - 茹でたうどんを器に盛り、その上から熱々のだし汁を注ぎます。\\n\\n5. **トッピング**:\\n   - お好みのトッピング（ネギ、天かす、かまぼこ、ほうれん草、すりごまなど）をのせて完成です。\\n\\n### おすすめのアレンジ\\n- **カレーうどん**: だし汁にカレー粉を加えて、カレーうどんにアレンジ。\\n- **冷やしうどん**: 冷たいだし汁やつけだしを用意して、冷やしうどんとして楽しむ。\\n\\nぜひ、お好みのトッピングでアレンジしてみてください！']\n"
          ]
        }
      ],
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "outputs = chain.batch([{\"dish\": \"カレー\"}, {\"dish\": \"うどん\"}])\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55ebzH6V9Jji"
      },
      "source": [
        "### LCEL の「|」で様々な Runnable を連鎖させる\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:38.984366Z",
          "iopub.status.busy": "2024-06-28T02:33:38.983620Z",
          "iopub.status.idle": "2024-06-28T02:33:39.072077Z",
          "shell.execute_reply": "2024-06-28T02:33:39.071585Z"
        },
        "id": "U5LiRICF9Jji"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.073954Z",
          "iopub.status.busy": "2024-06-28T02:33:39.073805Z",
          "iopub.status.idle": "2024-06-28T02:33:39.076746Z",
          "shell.execute_reply": "2024-06-28T02:33:39.076291Z"
        },
        "id": "HsU097S39Jji"
      },
      "outputs": [],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ユーザーの質問にステップバイステップで回答してください。\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cot_chain = cot_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.078597Z",
          "iopub.status.busy": "2024-06-28T02:33:39.078287Z",
          "iopub.status.idle": "2024-06-28T02:33:39.080804Z",
          "shell.execute_reply": "2024-06-28T02:33:39.080464Z"
        },
        "id": "jEcxawPk9Jji"
      },
      "outputs": [],
      "source": [
        "summarize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ステップバイステップで考えた回答から結論だけ抽出してください。\"),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "summarize_chain = summarize_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:39.082487Z",
          "iopub.status.busy": "2024-06-28T02:33:39.082344Z",
          "iopub.status.idle": "2024-06-28T02:33:42.144944Z",
          "shell.execute_reply": "2024-06-28T02:33:42.144538Z"
        },
        "id": "lNCM8K2B9Jji",
        "outputId": "9dc938b0-135e-474f-aaf1-1c00ad26de7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 + 2 * 3 の答えは **16** です。\n"
          ]
        }
      ],
      "source": [
        "cot_summarize_chain = cot_chain | summarize_chain\n",
        "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXf7UQv99Jji"
      },
      "source": [
        "## 5.2. RunnableLambda―任意の関数を Runnable にする\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:42.146898Z",
          "iopub.status.busy": "2024-06-28T02:33:42.146736Z",
          "iopub.status.idle": "2024-06-28T02:33:42.204154Z",
          "shell.execute_reply": "2024-06-28T02:33:42.203681Z"
        },
        "id": "h9k5l_Tb9Jjj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:42.206082Z",
          "iopub.status.busy": "2024-06-28T02:33:42.205909Z",
          "iopub.status.idle": "2024-06-28T02:33:43.030226Z",
          "shell.execute_reply": "2024-06-28T02:33:43.029756Z"
        },
        "id": "Q6ot8fO39Jjj",
        "outputId": "7fd44aea-baaf-44fa-968a-2919f8366f08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
        "\n",
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wtob-IM9Jjj"
      },
      "source": [
        "### chain デコレーターを使った RunnableLambda の実装\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.032211Z",
          "iopub.status.busy": "2024-06-28T02:33:43.032048Z",
          "iopub.status.idle": "2024-06-28T02:33:43.501555Z",
          "shell.execute_reply": "2024-06-28T02:33:43.499283Z"
        },
        "id": "O7Zgw3zJ9Jjj",
        "outputId": "2de12647-2a1c-4497-a8ba-9690821b43b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "\n",
        "@chain\n",
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | upper\n",
        "\n",
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuOHiW9W9Jjj"
      },
      "source": [
        "### RunnableLambda への自動変換\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.508518Z",
          "iopub.status.busy": "2024-06-28T02:33:43.508321Z",
          "iopub.status.idle": "2024-06-28T02:33:43.511080Z",
          "shell.execute_reply": "2024-06-28T02:33:43.510672Z"
        },
        "id": "fsN_sdxZ9Jjj"
      },
      "outputs": [],
      "source": [
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | output_parser | upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.512885Z",
          "iopub.status.busy": "2024-06-28T02:33:43.512594Z",
          "iopub.status.idle": "2024-06-28T02:33:43.961318Z",
          "shell.execute_reply": "2024-06-28T02:33:43.960803Z"
        },
        "id": "ez_Qihvw9Jjj",
        "outputId": "2555e6ae-f964-4ee1-e5f4-549db71ff693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
          ]
        }
      ],
      "source": [
        "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(ai_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHgyo1vK9Jjj"
      },
      "source": [
        "### Runnable の入力の型と出力の型に注意\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.963174Z",
          "iopub.status.busy": "2024-06-28T02:33:43.963026Z",
          "iopub.status.idle": "2024-06-28T02:33:43.965674Z",
          "shell.execute_reply": "2024-06-28T02:33:43.965305Z"
        },
        "id": "ndKLNK0l9Jjj"
      },
      "outputs": [],
      "source": [
        "def upper(text: str) -> str:\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | upper\n",
        "\n",
        "# 以下のコードを実行するとエラーになります\n",
        "# output = chain.invoke({\"input\": \"Hello!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:43.967542Z",
          "iopub.status.busy": "2024-06-28T02:33:43.967246Z",
          "iopub.status.idle": "2024-06-28T02:33:44.531734Z",
          "shell.execute_reply": "2024-06-28T02:33:44.531210Z"
        },
        "id": "5Bsqjln59Jjj"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | StrOutputParser() | upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2M72xuFU9Jjj",
        "outputId": "f6500641-310e-4759-af8a-f5973150a1a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
          ]
        }
      ],
      "source": [
        "output = chain.invoke({\"input\": \"Hello!\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d-IZ32m9Jjk"
      },
      "source": [
        "### （コラム）独自の関数を stream に対応させたい場合\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:44.533838Z",
          "iopub.status.busy": "2024-06-28T02:33:44.533678Z",
          "iopub.status.idle": "2024-06-28T02:33:45.353621Z",
          "shell.execute_reply": "2024-06-28T02:33:45.353115Z"
        },
        "id": "WmAbKrIq9Jjk",
        "outputId": "d6042ea1-f5b8-4a5e-f5c7-47c8d836f651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO! HOW CAN I ASSIST YOU TODAY?"
          ]
        }
      ],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "\n",
        "def upper(input_stream: Iterator[str]) -> Iterator[str]:\n",
        "    for text in input_stream:\n",
        "        yield text.upper()\n",
        "\n",
        "\n",
        "chain = prompt | model | StrOutputParser() | upper\n",
        "\n",
        "for chunk in chain.stream({\"input\": \"Hello!\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibG0wrEu9Jjk"
      },
      "source": [
        "## 5.3. RunnableParallel―複数の Runnable を並列で処理する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.355560Z",
          "iopub.status.busy": "2024-06-28T02:33:45.355402Z",
          "iopub.status.idle": "2024-06-28T02:33:45.407879Z",
          "shell.execute_reply": "2024-06-28T02:33:45.407407Z"
        },
        "id": "QdRCGY5L9Jjk"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.409670Z",
          "iopub.status.busy": "2024-06-28T02:33:45.409524Z",
          "iopub.status.idle": "2024-06-28T02:33:45.412232Z",
          "shell.execute_reply": "2024-06-28T02:33:45.411889Z"
        },
        "id": "cvNlgS-x9Jjk"
      },
      "outputs": [],
      "source": [
        "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"あなたは楽観主義者です。ユーザーの入力に対して楽観的な意見をください。\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")\n",
        "optimistic_chain = optimistic_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.414087Z",
          "iopub.status.busy": "2024-06-28T02:33:45.413807Z",
          "iopub.status.idle": "2024-06-28T02:33:45.416778Z",
          "shell.execute_reply": "2024-06-28T02:33:45.416359Z"
        },
        "id": "3fS2bNpc9Jjk"
      },
      "outputs": [],
      "source": [
        "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"あなたは悲観主義者です。ユーザーの入力に対して悲観的な意見をください。\"),\n",
        "        (\"human\", \"{topic}\"),\n",
        "    ]\n",
        ")\n",
        "pessimistic_chain = pessimistic_prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:45.418636Z",
          "iopub.status.busy": "2024-06-28T02:33:45.418458Z",
          "iopub.status.idle": "2024-06-28T02:33:48.115947Z",
          "shell.execute_reply": "2024-06-28T02:33:48.115444Z"
        },
        "id": "IkNVivNz9Jjk",
        "outputId": "a7b6494a-e670-44ad-fe37-036ec7b8c0c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'optimistic_opinion': '生成AIの進化は本当に素晴らしいですね！技術が進むことで、私たちの生活がより便利で豊かになる可能性が広がっています。クリエイティブな作業や問題解決の手助けをしてくれるAIが増えてきて、私たちのアイデアを実現するためのパートナーとして活躍しています。これからも新しい発見や革新が続くことで、私たちの未来はますます明るくなるでしょう！どんな新しい可能性が待っているのか、ワクワクしますね。',\n",
            " 'pessimistic_opinion': '生成AIの進化は確かに目覚ましいものがありますが、その一方で多くの懸念も伴っています。技術が進化することで、私たちの仕事が奪われたり、情報の信頼性が低下したりするリスクが高まっています。さらに、AIが生成するコンテンツが増えることで、オリジナリティやクリエイティビティが失われる可能性もあります。結局のところ、便利さの裏には多くの問題が潜んでいるのではないでしょうか。私たちがこの技術をどのように扱うかが、未来に大きな影響を与えることは間違いありませんが、その未来が明るいとは限らないのが現実です。'}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "parallel_chain = RunnableParallel(\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        ")\n",
        "\n",
        "output = parallel_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
        "pprint.pprint(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS76KfD79Jjk"
      },
      "source": [
        "### RunnableParallel の出力を Runnable の入力に連結する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:48.117879Z",
          "iopub.status.busy": "2024-06-28T02:33:48.117728Z",
          "iopub.status.idle": "2024-06-28T02:33:54.979992Z",
          "shell.execute_reply": "2024-06-28T02:33:54.978363Z"
        },
        "id": "KyJgKeC09Jjk"
      },
      "outputs": [],
      "source": [
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"あなたは客観的AIです。2つの意見をまとめてください。\"),\n",
        "        (\"human\", \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3moYy4Lq9Jjk",
        "outputId": "a40e65a6-6a31-4e46-8826-72ef0a98749a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成AIの進化については、楽観的な意見と悲観的な意見が存在します。楽観的な見方では、生成AIの技術が進化することで、私たちの生活が便利で豊かになる可能性が広がり、クリエイティブな作業や問題解決のパートナーとしての役割を果たすことが期待されています。この進化により、新しい発見や革新が続き、未来が明るくなることへの期待感が高まっています。\n",
            "\n",
            "一方で、悲観的な見方では、生成AIの進化には多くの懸念が伴い、仕事の喪失や情報の信頼性の低下といったリスクが増大することが指摘されています。また、AIが生成するコンテンツが人間の創造性を脅かし、思考や感情に悪影響を及ぼす可能性も懸念されています。このように、便利さの裏には不安がつきまとい、未来が不透明になるのではないかという懸念も存在します。\n",
            "\n",
            "総じて、生成AIの進化は多くの可能性を秘めている一方で、リスクや懸念も無視できないという複雑な状況が浮かび上がります。\n"
          ]
        }
      ],
      "source": [
        "synthesize_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"optimistic_opinion\": optimistic_chain,\n",
        "            \"pessimistic_opinion\": pessimistic_chain,\n",
        "        }\n",
        "    )\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2LN-XXi9Jjk"
      },
      "source": [
        "### RunnableParallel への自動変換\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:54.986651Z",
          "iopub.status.busy": "2024-06-28T02:33:54.986098Z",
          "iopub.status.idle": "2024-06-28T02:33:54.994428Z",
          "shell.execute_reply": "2024-06-28T02:33:54.992735Z"
        },
        "id": "mZpNRqbJ9Jjl"
      },
      "outputs": [],
      "source": [
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:33:55.000866Z",
          "iopub.status.busy": "2024-06-28T02:33:55.000312Z",
          "iopub.status.idle": "2024-06-28T02:34:01.170210Z",
          "shell.execute_reply": "2024-06-28T02:34:01.169705Z"
        },
        "id": "d-PElo8T9Jjl",
        "outputId": "8807cad3-806d-461a-e422-91b4a844c0c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成AIの進化については、楽観的な意見と悲観的な意見が存在します。楽観的な意見では、生成AIが私たちの生活を便利で豊かにし、クリエイティブな作業や問題解決のパートナーとしての役割を果たすことが期待されています。また、教育や医療、エンターテインメントなどの分野での革新を促進し、より多くの人々が新しい知識や体験にアクセスできるようになることが、社会全体の進化につながるとされています。\n",
            "\n",
            "一方で、悲観的な意見では、生成AIの進化が仕事の喪失や情報の信頼性の低下を引き起こすリスクがあると指摘されています。AIが生成するコンテンツの氾濫により、真実と虚偽の区別が難しくなり、社会が混乱する可能性も懸念されています。このように、技術の進化は必ずしも良い結果をもたらすわけではなく、新たな問題を引き起こす要因となるかもしれません。\n",
            "\n",
            "総じて、生成AIの進化には多くの可能性と同時にリスクも伴うため、慎重な対応が求められています。\n"
          ]
        }
      ],
      "source": [
        "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjzhFe1u9Jjl"
      },
      "source": [
        "### RunnableLambda との組み合わせ―itemgetter を使う例\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:01.172136Z",
          "iopub.status.busy": "2024-06-28T02:34:01.171982Z",
          "iopub.status.idle": "2024-06-28T02:34:01.174756Z",
          "shell.execute_reply": "2024-06-28T02:34:01.174370Z"
        },
        "id": "iQmWvzj39Jjl",
        "outputId": "a709ef9c-1624-4abb-b9d7-3ad3a4d41ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成AIの進化について\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "topic_getter = itemgetter(\"topic\")\n",
        "topic = topic_getter({\"topic\": \"生成AIの進化について\"})\n",
        "print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:01.176493Z",
          "iopub.status.busy": "2024-06-28T02:34:01.176278Z",
          "iopub.status.idle": "2024-06-28T02:34:09.682638Z",
          "shell.execute_reply": "2024-06-28T02:34:09.682146Z"
        },
        "id": "kFwcpZoF9Jjl",
        "outputId": "d1785fb8-bf06-471c-ddd3-d1daa2c6bb04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成AIの進化に関する意見は、楽観的な見方と悲観的な見方の2つに分かれます。\n",
            "\n",
            "**楽観的意見:** 生成AIの進化は、私たちの生活をより便利で豊かにする可能性を秘めています。新しい技術が登場することで、クリエイティブな作業や問題解決の支援が行われ、私たちのアイデアや想像力を引き出す手助けをしてくれるでしょう。未来において、生成AIがどのように社会を変革していくのか、期待と興奮を持って見守ることができます。\n",
            "\n",
            "**悲観的意見:** 一方で、生成AIの進化には多くの懸念も伴います。技術の進化が進むことで、仕事の喪失や情報の信頼性の低下といったリスクが増大します。また、AIが生成するコンテンツが氾濫することで、真実と虚偽の区別が難しくなり、社会全体が混乱する可能性もあります。このように、進化が必ずしも良い結果をもたらすわけではないという視点も重要です。\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"あなたは客観的AIです。{topic}について2つの意見をまとめてください。\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"楽観的意見: {optimistic_opinion}\\n悲観的意見: {pessimistic_opinion}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "synthesize_chain = (\n",
        "    {\n",
        "        \"optimistic_opinion\": optimistic_chain,\n",
        "        \"pessimistic_opinion\": pessimistic_chain,\n",
        "        \"topic\": itemgetter(\"topic\"),\n",
        "    }\n",
        "    | synthesize_prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "output = synthesize_chain.invoke({\"topic\": \"生成AIの進化について\"})\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWmaZ0Cw9Jjl"
      },
      "source": [
        "## 5.4. RunnablePassthrough―入力をそのまま出力する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Vgpu8sLk9Jjl",
        "outputId": "36733b5e-808a-4e2d-aabd-de4a5d419026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret TAVILY_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-39ad57627a3c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TAVILY_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TAVILY_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret TAVILY_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXL0-Mlw9Jjl"
      },
      "outputs": [],
      "source": [
        "!pip install tavily-python==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I14T6Usg9Jjl"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "以下の文脈だけを踏まえて質問に回答してください。\n",
        "\n",
        "文脈: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "質問: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXiC2n1T9Jjl"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
        "\n",
        "retriever = TavilySearchAPIRetriever(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TchGJFUz9Jjl"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "output = chain.invoke(\"東京の今日の天気は？\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT7sjPsB9Jjm"
      },
      "source": [
        "### assign―RunnableParallel に値を追加する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:14.820735Z",
          "iopub.status.busy": "2024-06-28T02:34:14.820536Z",
          "iopub.status.idle": "2024-06-28T02:34:20.477887Z",
          "shell.execute_reply": "2024-06-28T02:34:20.477378Z"
        },
        "id": "F8067xMM9Jjm"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": retriever,\n",
        "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
        "\n",
        "output = chain.invoke(\"東京の今日の天気は？\")\n",
        "pprint.pprint(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:20.479839Z",
          "iopub.status.busy": "2024-06-28T02:34:20.479664Z",
          "iopub.status.idle": "2024-06-28T02:34:27.723756Z",
          "shell.execute_reply": "2024-06-28T02:34:27.723236Z"
        },
        "id": "24ldM6U59Jjm"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain = RunnableParallel(\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"context\": retriever,\n",
        "    }\n",
        ").assign(answer=prompt | model | StrOutputParser())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG5rz86C9Jjm"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"東京の今日の天気は？\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqN49qwo9Jjm"
      },
      "source": [
        "#### ＜補足：pick ＞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:27.725885Z",
          "iopub.status.busy": "2024-06-28T02:34:27.725708Z",
          "iopub.status.idle": "2024-06-28T02:34:34.101909Z",
          "shell.execute_reply": "2024-06-28T02:34:34.101439Z"
        },
        "id": "Esjw0Yqh9Jjm"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"context\": retriever,\n",
        "        }\n",
        "    )\n",
        "    .assign(answer=prompt | model | StrOutputParser())\n",
        "    .pick([\"context\", \"answer\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eV6QbQF9Jjm"
      },
      "outputs": [],
      "source": [
        "output = chain.invoke(\"東京の今日の天気は？\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GubsSOmn9Jjm"
      },
      "source": [
        "### （コラム）astream_events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SeKYh-L9Jjm"
      },
      "outputs": [],
      "source": [
        "# Google Colabでは次のコードの「async」の箇所に「Use of \"async\" not allowed outside of async function」と表示されますが、エラーなく実行できます"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:34:34.103973Z",
          "iopub.status.busy": "2024-06-28T02:34:34.103802Z",
          "iopub.status.idle": "2024-06-28T02:34:34.107769Z",
          "shell.execute_reply": "2024-06-28T02:34:34.107274Z"
        },
        "id": "9DQkxHft9Jjm"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "async for event in chain.astream_events(\"東京の今日の天気は？\", version=\"v2\"):\n",
        "    print(event, flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mokpCsQa9Jjm"
      },
      "outputs": [],
      "source": [
        "async for event in chain.astream_events(\"東京の今日の天気は？\", version=\"v2\"):\n",
        "    event_kind = event[\"event\"]\n",
        "\n",
        "    if event_kind == \"on_retriever_end\":\n",
        "        print(\"=== 検索結果 ===\")\n",
        "        documents = event[\"data\"][\"output\"]\n",
        "        for document in documents:\n",
        "            print(document)\n",
        "\n",
        "    elif event_kind == \"on_parser_start\":\n",
        "        print(\"=== 最終出力 ===\")\n",
        "\n",
        "    elif event_kind == \"on_parser_stream\":\n",
        "        chunk = event[\"data\"][\"chunk\"]\n",
        "        print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArM-Hibo9Jjm"
      },
      "source": [
        "### （コラム）Chat history と Memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHrRDwqY9Jjn"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxE-QCfp9Jjn"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "\n",
        "\n",
        "def respond(session_id: str, human_message: str) -> str:\n",
        "    chat_message_history = SQLChatMessageHistory(\n",
        "        session_id=session_id, connection=\"sqlite:///sqlite.db\"\n",
        "    )\n",
        "\n",
        "    ai_message = chain.invoke(\n",
        "        {\n",
        "            \"chat_history\": chat_message_history.get_messages(),\n",
        "            \"input\": human_message,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    chat_message_history.add_user_message(human_message)\n",
        "    chat_message_history.add_ai_message(ai_message)\n",
        "\n",
        "    return ai_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQDKEo359Jjn"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "session_id = uuid4().hex\n",
        "\n",
        "output1 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"こんにちは！私はジョンと言います！\",\n",
        ")\n",
        "print(output1)\n",
        "\n",
        "output2 = respond(\n",
        "    session_id=session_id,\n",
        "    human_message=\"私の名前が分かりますか？\",\n",
        ")\n",
        "print(output2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}